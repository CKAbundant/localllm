"""Class to compare local LLM ratings with ground truth i.e.
ratings generated by Perplexity Sonar API."""

import json
from pathlib import Path

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import seaborn as sns
from omegaconf import DictConfig
from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score
from tqdm import tqdm

from src.local_llm import InferLLM
from src.utils import utils


class GenAnalysis:
    """Compare local LLM ratings with ground truth:

    - Generate ratings via selected local LLM using news items in test dataset.
    - Generate F1 Score, Precision, Recall and confusion matrix using
    local llm's ratings (predicted) and Perplexity Sonar API (ground truth).

    Usage:
        >>> cfg = utils.load_config() # Load configuration from 'config.yaml'
        >>> cfg_model = cfg[cfg.model] # Get configuration for specific LLM
        >>> gen_analysis = GenAnalysis(
                infer_llm,
                cfg.model,
                cfg.path,
                cfg_model.ratings_path,
                cfg_model.metrics_path,
                cfg.req_cols,
            )
        >>> df_senti, df_metrics = gen_analysis.run()

    Args:
        local_llm (InferLLM):
            Initialized concrete implementatino of 'InferLLM' abstract class.
        model_name (str):
            Name of local llm.
        path (DictConfig):
            OmegaConf DictConfig containing required file and directory paths.
        fig (DictConfig):
            OmegaConf DictConfig containing settings for plotting confusion matrix.
        ratings_path (str):
            Relative path to csv file containing generated sentiment ratings
            by local llm.
        metrics_path (str):
            Relative path to csv file containing performance metrics for local llm.
        req_cols (list[str]):
            List of columns to be displayed (Default: ["id", "pub_date", "ticker",
            "title", "content", "rating", "reasons"]).

    Attributes:
        local_llm (InferLLM):
            Initialized concrete implementatino of 'InferLLM' abstract class.
        model_name (str):
            Name of local llm.
        fig (DictConfig):
            OmegaConf DictConfig containing settings for plotting confusion matrix.
        ratings_path (str):
            Relative path to csv file containing generated sentiment ratings
            by local llm.
        metrics_path (str):
            Relative path to csv file containing performance metrics for local llm.
        fig_path (str):
            Relative path to confusion matrix plot.
        test_path (str):
            Relative path to test dataset (Default: "./data/test.csv).
        req_cols (list[str]):
            List of columns to be displayed (Default: ["id", "pub_date", "ticker",
            "title", "content", "rating", "reasons"]).
    """

    def __init__(
        self,
        local_llm: InferLLM,
        model_name: str,
        path: DictConfig,
        fig: DictConfig,
        ratings_path: str,
        metrics_path: str,
        req_cols: list[str] = [
            "id",
            "pub_date",
            "ticker",
            "title",
            "content",
            "rating",
            "reasons",
        ],
    ) -> None:
        self.local_llm = local_llm
        self.model_name = model_name
        self.fig = fig
        self.ratings_path = ratings_path
        self.metrics_path = metrics_path
        self.fig_path = (
            f"{path.data_dir}/{model_name.split("_")[0]}/cm_{model_name}.png"
        )
        self.test_path = path.test
        self.req_cols = req_cols

    def run(self) -> tuple[pd.DataFrame, pd.DataFrame]:
        """Generate sentiment ratings by local LLM and evaluate its performance.

        Args:
            None.

        Returns:
            df_senti (pd.DataFrame):
                DataFrame containing sentiment ratings by local LLM.
            df_metrics (pd.DataFrame):
                DataFrame containing performance metrics for local LLM.
        """

        # Generate DataFrame containing sentiment ratings by local LLM
        df_senti = self.gen_dataset()

        # df_senti = pd.read_csv(self.ratings_path)

        # Generate DataFrame containing performance metrics
        df_metrics = self.gen_metrics(df_senti)

        return df_senti, df_metrics

    def gen_dataset(self) -> pd.DataFrame:
        """Generate sentiment ratings by local llm and return as DataFrame."""

        if not Path(self.test_path).is_file():
            raise FileNotFoundError(f"'test.csv' not found at {self.test_path}")

        # Load 'test.csv' as DataFrame
        df_test = pd.read_csv(self.test_path)

        # Generate news_list
        news_list = self.gen_news_list(df_test)

        # Generate list of sentiment ratings and reasons based on 'news_list'

        rating_list = []
        for news_item in tqdm(news_list):
            rating_list.append(self.local_llm.senti_rate(news_item))
            print(f"self.local_llm.timings : {self.local_llm.timings}")

        # Convert to 'rating_list' to DataFrame
        df_ratings = pd.DataFrame(rating_list)

        # Merge df_test with rating_list
        df_filter = df_test.loc[:, self.req_cols]
        df_senti = df_filter.merge(
            df_ratings, how="left", on="id", suffixes=(None, f"_{self.model_name}")
        )
        df_senti["infer_time"] = self.local_llm.timings

        # Create folder if not exist
        utils.create_folder(Path(self.ratings_path).parent)

        # Save 'df_senti' as csv file
        df_senti.to_csv(self.ratings_path, index=False)

        return df_senti

    def gen_news_list(self, df_news: pd.DataFrame) -> list[dict[str, int | str]]:
        """Generate list of news items i.e. dictionary containing 'id', 'ticker'
        # and 'content' keys."""

        df = df_news.copy()

        # Combine 'title' and 'content' to 'news' column
        df["news"] = df["title"] + "\n\n" + df["content"]

        # Filter 'id', 'ticker' and 'news' columns
        df = df.loc[:, ["id", "ticker", "news"]]

        # Convert to list of dictionary
        return df.to_dict(orient="records")

    def gen_metrics(self, df_senti: pd.DataFrame) -> pd.DataFrame:
        """Generate F1 score, precision, recall and confusion matrix.

        Args:
            df_senti (pd.DataFrame):
                DataFrame containing sentiment ratings generated by local LLM
                and Perplexity API.

        Returns:
            df_metrics (pd.DataFrame):
                DataFrame with 'model', 'f1_score', 'precision', 'recall',
                'total_time', 'mean_time' and 'cmatrix'.
        """

        y_true = df_senti["rating"]
        y_pred = df_senti[f"rating_{self.model_name}"]

        # Convert confusion matrix to a json string
        cmatrix = confusion_matrix(y_true, y_pred)
        cmatrix_str = json.dumps(cmatrix.tolist())

        # Get list of elapsed timing for inference
        timings = self.local_llm.timings

        metrics = {
            "f1_score": f1_score(y_true, y_pred, average="macro"),
            "precision": precision_score(y_true, y_pred, average="macro"),
            "recall": recall_score(y_true, y_pred, average="macro"),
            "total_time": np.sum(timings),
            "mean_time": np.mean(timings) if len(timings) > 0 else 0,
            "cmatrix": cmatrix_str,
        }

        # Convert to DataFrame
        df_metrics = pd.DataFrame([metrics])
        df_metrics.to_csv(self.metrics_path)

        # Save confusion matrix as image
        self.plot_cmatrix(cmatrix)

        return df_metrics

    def plot_cmatrix(self, cmatrix: np.ndarray) -> None:
        """Plot confusion matrix as seaborn heatmap."""

        # Generate labels for ratings 1 to 5
        idx_labels = [f"Act {i+1}" for i in range(5)]
        col_labels = [f"Pred {i+1}" for i in range(5)]

        # Convert confusion matrix to DataFrame
        df_cm = pd.DataFrame(cmatrix, index=idx_labels, columns=col_labels)

        # Set theme for sns plot
        sns.set_theme(**self.fig.theme)

        # Plot confusion matrix as seaborn heatmap
        fig, ax = plt.subplots(figsize=(12, 10))

        ax = sns.heatmap(df_cm, **self.fig.heatmap)
        ax.set_xlabel("Predicted Sentiment Rating", fontsize=18)
        ax.set_ylabel("Actual Sentiment Rating", fontsize=18)
        ax.set_title(f"Confusion Matrix for {self.model_name}", fontsize=24)

        # Save figure as png file
        plt.tight_layout()
        fig.savefig(self.fig_path)
        plt.close()

"""Class to compare local LLM ratings with ground truth i.e.
ratings generated by Perplexity Sonar API."""

import csv
import json
from pathlib import Path
from pprint import pformat

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import seaborn as sns
from omegaconf import DictConfig
from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score
from tqdm import tqdm

from src.local_llm import InferLLM
from src.utils import utils


class GenAnalysis:
    """Compare local LLM ratings with ground truth:

    - Generate ratings via selected local LLM using news items in test dataset.
    - Generate F1 Score, Precision, Recall and confusion matrix using
    local llm's ratings (predicted) and Perplexity Sonar API (ground truth).

    Usage:
        >>> cfg = utils.load_config() # Load configuration from 'config.yaml'
        >>> cfg_model = cfg[cfg.model] # Get configuration for specific LLM
        >>> gen_analysis = GenAnalysis(
                infer_llm,
                cfg.model,
                cfg.path,
                cfg_model.ratings_path,
                cfg_model.metrics_path,
                cfg.req_cols,
            )
        >>> df_senti, df_metrics = gen_analysis.run()

    Args:
        local_llm (InferLLM):
            Initialized concrete implementatino of 'InferLLM' abstract class.
        model_name (str):
            Name of local llm.
        path (DictConfig):
            OmegaConf DictConfig containing required file and directory paths.
        fig (DictConfig):
            OmegaConf DictConfig containing settings for plotting confusion matrix.
        ratings_path (str):
            Relative path to csv file containing generated sentiment ratings
            by local llm.
        metrics_path (str):
            Relative path to csv file containing performance metrics for local llm.
        req_cols (list[str]):
            List of columns to be displayed (Default: ["id", "pub_date", "ticker",
            "title", "content", "rating", "reasons"]).
        start_id (int):
            ID of the news item to start sentiment rating instead of starting
            from scratch (Default: 0).
        batch_size (int):
            Number of news items to rate sentiment in single batch.
            No batch sentiment analysis if None.

    Attributes:
        local_llm (InferLLM):
            Initialized concrete implementatino of 'InferLLM' abstract class.
        model_name (str):
            Name of local llm.
        fig (DictConfig):
            OmegaConf DictConfig containing settings for plotting confusion matrix.
        ratings_path (str):
            Relative path to csv file containing generated sentiment ratings
            by local llm.
        metrics_path (str):
            Relative path to csv file containing performance metrics for local llm.
        fig_path (str):
            Relative path to confusion matrix plot.
        test_path (str):
            Relative path to test dataset (Default: "./data/test.csv).
        req_cols (list[str]):
            List of columns to be displayed (Default: ["id", "pub_date", "ticker",
            "title", "content", "rating", "reasons"]).
        start_id (int):
            ID of the news item to start sentiment rating instead of starting
            from scratch (Default: 0).
        batch_size (int):
            Number of news items to rate sentiment in single batch.
            No batch sentiment analysis if None.
    """

    def __init__(
        self,
        local_llm: InferLLM,
        model_name: str,
        path: DictConfig,
        fig: DictConfig,
        ratings_path: str,
        metrics_path: str,
        req_cols: list[str] = [
            "id",
            "pub_date",
            "ticker",
            "title",
            "content",
            "rating",
            "reasons",
        ],
        start_id: int = 0,
        batch_size: int | None = None,
    ) -> None:
        self.local_llm = local_llm
        self.model_name = model_name
        self.fig = fig
        self.ratings_path = ratings_path
        self.metrics_path = metrics_path
        self.fig_path = (
            f"{path.data_dir}/{model_name.split("_")[0]}/cm_{model_name}.png"
        )
        self.test_path = path.test
        self.req_cols = req_cols
        self.start_id = start_id
        self.batch_size = batch_size

    def run(self) -> tuple[pd.DataFrame, pd.DataFrame]:
        """Generate sentiment ratings by local LLM and evaluate its performance.

        Args:
            None.

        Returns:
            df_senti (pd.DataFrame):
                DataFrame containing sentiment ratings by local LLM.
            df_metrics (pd.DataFrame):
                DataFrame containing performance metrics for local LLM.
        """

        df_senti = self.gen_dataset()
        df_metrics = self.gen_metrics(df_senti)

        return df_senti, df_metrics

    def single_senti(self, news_list: list[dict[str, int]], start_idx: int) -> str:
        """Generate sentiment ratings without batching; and save as csv file."""

        # Create temp file to save ratings and check if exist
        temp_path = Path(self.ratings_path).with_stem(self.model_name)
        temp_exist = temp_path.is_file()

        with open(temp_path, "a", newline="") as csvfile:
            # Create DictWriter and write column names
            writer = csv.DictWriter(
                csvfile, fieldnames=["id", "rating", "reasons", "infer_time"]
            )

            if not temp_exist:
                writer.writeheader()

            for news_item in tqdm(news_list[start_idx:]):
                # Generate 'ratings' dictionary containing 'id', 'rating' and 'reasons'
                ratings = self.local_llm.senti_rate(news_item)

                # Append infer time to 'ratings' dictionary
                elapsed = self.local_llm.timings[-1]
                ratings["infer_time"] = elapsed
                print(f"elapsed : {elapsed}\n")

                # Ensure immediate write to disc
                writer.writerow(ratings)
                csvfile.flush()

        return temp_path

    def batch_senti(self, news_list: list[dict[str, int]], start_idx: int) -> str:
        """Generate sentiment ratings with batching; and save as csv file."""

        print(f"\nPerforming batch sentiment analysis...\n")

        # Create temp file to save ratings and check if exist
        temp_path = Path(self.ratings_path).with_stem(f"{self.model_name}_batch")
        temp_exist = temp_path.is_file()

        with open(temp_path, "a", newline="") as csvfile:
            # Create DictWriter and write column names
            writer = csv.DictWriter(
                csvfile, fieldnames=["id", "rating", "reasons", "infer_time"]
            )

            if not temp_exist:
                writer.writeheader()

            updated_list = news_list[start_idx:]

            for idx in tqdm(range(0, len(updated_list), self.batch_size)):
                # Generate batch of news items of size 'batch_size'
                batch_news = updated_list[idx : idx + self.batch_size]
                print(f"batch_news [{idx}] : \n\n{pformat(batch_news)}\n")

                # Generate 'ratings' dictionary containing 'id', 'rating' and 'reasons'
                ratings_list = self.local_llm.batch_senti_rate(batch_news)

                if len(ratings_list) == 0:
                    continue

                # Update each ratings in 'ratings_list' with average elapsed time
                ratings_list = self.update_elapsed(ratings_list, self.batch_size)

                # Ensure immediate write to disc
                writer.writerows(ratings_list)
                csvfile.flush()

        return temp_path

    def gen_dataset(self) -> pd.DataFrame:
        """Generate sentiment ratings by local llm in batches and return as
        DataFrame."""

        if not Path(self.test_path).is_file():
            raise FileNotFoundError(f"'test.csv' not found at {self.test_path}")

        # Load 'test.csv' as DataFrame
        df_test = pd.read_csv(self.test_path)

        # Generate news_list
        news_list = self.gen_news_list(df_test)

        # Create folder if not exist
        utils.create_folder(Path(self.ratings_path).parent)

        # Get start index
        start_idx = self.get_start_index(news_list)

        # No batch processing if 'self.batch_size' is None
        if self.batch_size is None:
            temp_path = self.single_senti(news_list, start_idx)
        else:
            temp_path = self.batch_senti(news_list, start_idx)

        # Load DataFrame generated by csv.DictWriter
        df_ratings = pd.read_csv(temp_path)

        # Remove duplicates 'id'
        df_ratings = df_ratings.drop_duplicates(subset="id").reset_index(drop=True)

        # Merge df_test with rating_list
        df_filter = df_test.loc[:, self.req_cols]
        df_senti = df_filter.merge(
            df_ratings, how="left", on="id", suffixes=(None, f"_{self.model_name}")
        )

        # Save 'df_senti' as csv file
        df_senti.to_csv(self.ratings_path, index=False)

        return df_senti

    def update_elapsed(
        self, ratings_list: list[dict[str, int | str]], batch_size: int
    ) -> list[dict[str, int | str]]:
        """Append average elapsed time to each ratings in 'ratings_list'.

        Args:
            ratings_list (list[dict[str, int | str]]):
                List of 'ratings' dictionary containing 'id', 'rating' and 'reasons'
                generated by local LLM.
            batch_size (int):
                Number of news items to rate sentiment in single batch.

        Returns:
            updated_list (list[dict[str, int | str]]):
                List of 'ratings' dictionary with updated average elapsed time.
        """

        # Average time elapsed per news item during batch inferencing
        elapsed = self.local_llm.timings[-1] / batch_size

        print(
            "Average elapsed time per news item [batch_size: "
            f"{batch_size}] : {elapsed}\n"
        )

        updated_list = []
        for ratings in ratings_list:
            if not isinstance(ratings, dict):
                continue

            ratings["infer_time"] = elapsed
            updated_list.append(ratings)

        return updated_list

    def gen_news_list(self, df_news: pd.DataFrame) -> list[dict[str, int | str]]:
        """Generate list of news items i.e. dictionary containing 'id', 'ticker'
        # and 'content' keys."""

        df = df_news.copy()

        # Combine 'title' and 'content' to 'news' column
        df["news"] = df["title"] + "\n\n" + df["content"]

        # Filter 'id', 'ticker' and 'news' columns
        df = df.loc[:, ["id", "ticker", "news"]]

        # Convert to list of dictionary
        return df.to_dict(orient="records")

    def gen_metrics(self, df_senti: pd.DataFrame) -> pd.DataFrame:
        """Generate F1 score, precision, recall and confusion matrix.

        Args:
            df_senti (pd.DataFrame):
                DataFrame containing sentiment ratings generated by local LLM
                and Perplexity API.

        Returns:
            df_metrics (pd.DataFrame):
                DataFrame with 'model', 'f1_score', 'precision', 'recall',
                'total_time', 'mean_time' and 'cmatrix'.
        """

        y_true = df_senti["rating"]
        y_pred = df_senti[f"rating_{self.model_name}"]

        # Convert confusion matrix to a json string
        cmatrix = confusion_matrix(y_true, y_pred)
        cmatrix_str = json.dumps(cmatrix.tolist())

        # Get list of elapsed timing for inference
        timings = self.local_llm.timings

        metrics = {
            "f1_score": f1_score(y_true, y_pred, average="macro"),
            "precision": precision_score(y_true, y_pred, average="macro"),
            "recall": recall_score(y_true, y_pred, average="macro"),
            "total_time": np.sum(timings),
            "mean_time": np.mean(timings) if len(timings) > 0 else 0,
            "cmatrix": cmatrix_str,
        }

        # Convert to DataFrame
        df_metrics = pd.DataFrame([metrics])
        df_metrics.to_csv(self.metrics_path, index=False)

        # Save confusion matrix as image
        self.plot_cmatrix(cmatrix)

        return df_metrics

    def plot_cmatrix(self, cmatrix: np.ndarray) -> None:
        """Plot confusion matrix as seaborn heatmap."""

        # Generate labels for ratings 1 to 5
        idx_labels = [f"Act {i+1}" for i in range(5)]
        col_labels = [f"Pred {i+1}" for i in range(5)]

        # Convert confusion matrix to DataFrame
        df_cm = pd.DataFrame(cmatrix, index=idx_labels, columns=col_labels)

        # Set theme for sns plot
        sns.set_theme(**self.fig.theme)

        # Plot confusion matrix as seaborn heatmap
        fig, ax = plt.subplots(figsize=(12, 10))

        ax = sns.heatmap(df_cm, **self.fig.heatmap)
        ax.set_xlabel("Predicted Sentiment Rating", fontsize=18)
        ax.set_ylabel("Actual Sentiment Rating", fontsize=18)
        ax.set_title(f"Confusion Matrix for {self.model_name}", fontsize=24)

        # Save figure as png file
        plt.tight_layout()
        fig.savefig(self.fig_path)
        plt.close()

    def get_start_index(self, news_list: list[dict[str, int | str]]) -> int:
        """Get the index of 'start_id' in 'news_item' list.

        Args:
            news_list (list[dict[str, int | str]]):
                List of news item which is a dictionary containing 'id', 'ticker',
                and 'news'.

        Returns:
            (int): index of selected news item based on 'start_id'
        """

        # Convert to DataFrame for ease of manipulation
        df = pd.DataFrame(news_list)

        # Return index for 'start_id'
        return df.loc[df["id"] == self.start_id, "id"].index.item()

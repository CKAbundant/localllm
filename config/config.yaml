gen_test_data: false
download: true
infer: true
model: "gemma_12b"

gen_analysis:
  model_name: "${model}"
  path: "${path}"
  fig: "${fig}"
  ratings_path: "${${model}.ratings_path}"
  metrics_path: "${${model}.metrics_path}"
  req_cols:
    - "id"
    - "pub_date"
    - "ticker"
    - "title"
    - "content"
    - "rating"
    - "reasons"
  start_id: 0
  batch_size: null # No batch sentiment analysis if null

# Standard Infer parameters
std_infer: &std_infer
  n_ctx: 2048
  script_path: "${path.script}"
  n_threads: 8
  n_gpu_layers: 0
  verbose: false

# Directory path
path:
  model_dir: "./models"
  data_dir: "./data"
  divergent: "${path.data_dir}/divergent.csv"
  test: "${path.data_dir}/test.csv"
  script: "./src/local_llm/local_llm.py"

# Graph settings
fig:
  theme:
      style: "whitegrid"
      palette: "pastel"
      font_scale: 1.5
  heatmap:
      annot: true # Display values in confusion matrix
      fmt: "d" # Display values as integer
      cmap: "Blues" # Seaborn color map
      cbar: true # Display color bar


########################
# Gemma
########################

gemma:
  family: "gemma"
  data_dir: "${path.data_dir}/${gemma.family}"
  model_dir: "${path.model_dir}/${gemma.family}"

  infer: &gemma_infer
    class_name: "LlamaLLM"
    temperature: 0.2

########################

gemma_1b_bf16:
  repo_id: "unsloth/gemma-3-1b-it-GGUF"
  filename: "gemma-3-1b-it-BF16.gguf"
  ratings_path: "${gemma.data_dir}/ratings_gemma_1b_bf16.csv"
  metrics_path: "${gemma.data_dir}/metrics_gemma_1b_bf16.csv"

  download:
    repo_id: "${gemma_1b_bf16.repo_id}"
    filename: "${gemma_1b_bf16.filename}"
    model_dir: "${gemma.model_dir}"
    token: "${oc.env:HF_KEY}"

  infer:
    model_path: "${gemma.model_dir}/${gemma_1b_bf16.filename}"
    <<: *std_infer
    <<: *gemma_infer

gemma_4b:
  repo_id: "google/gemma-3-4b-it-qat-q4_0-gguf"
  filename: "gemma-3-4b-it-q4_0.gguf"
  ratings_path: "${gemma.data_dir}/ratings_gemma_4b.csv"
  metrics_path: "${gemma.data_dir}/metrics_gemma_4b.csv"

  download:
    repo_id: "${gemma_4b.repo_id}"
    filename: "${gemma_4b.filename}"
    model_dir: "${gemma.model_dir}"
    token: "${oc.env:HF_KEY}"

  infer:
    model_path: "${gemma.model_dir}/${gemma_4b.filename}"
    <<: *std_infer
    <<: *gemma_infer

gemma_12b:
  repo_id: "google/gemma-3-12b-it-qat-q4_0-gguf"
  filename: "gemma-3-12b-it-q4_0.gguf" # 8.07 GB
  ratings_path: "${gemma.data_dir}/ratings_gemma_12b.csv"
  metrics_path: "${gemma.data_dir}/metrics_gemma_12b.csv"

  download:
    repo_id: "${gemma_12b.repo_id}"
    filename: "${gemma_12b.filename}"
    model_dir: "${gemma.model_dir}"
    token: "${oc.env:HF_KEY}"

  infer:
    model_path: "${gemma.model_dir}/${gemma_12b.filename}"
    <<: *std_infer
    <<: *gemma_infer

gemma_27b:
  repo_id: "google/gemma-3-27b-it-qat-q4_0-gguf"
  filename: "gemma-3-27b-it-q4_0.gguf" # 17.2GB -> 15.7GB RAM
  ratings_path: "${gemma.data_dir}/ratings_gemma_27b.csv"
  metrics_path: "${gemma.data_dir}/metrics_gemma_27b.csv"

  download:
    repo_id: "${gemma_27b.repo_id}"
    filename: "${gemma_27b.filename}"
    model_dir: "${gemma.model_dir}"
    token: "${oc.env:HF_KEY}"

  infer:
    model_path: "${gemma.model_dir}/${gemma_27b.filename}"
    <<: *std_infer
    <<: *gemma_infer

gemma_27b_q6:
  repo_id: "MaziyarPanahi/gemma-3-27b-it-GGUF"
  filename: "gemma-3-27b-it.Q6_K.gguf" # 17.2GB -> 15.7GB RAM
  ratings_path: "${gemma.data_dir}/ratings_gemma_27b_q6.csv"
  metrics_path: "${gemma.data_dir}/metrics_gemma_27b_q6.csv"

  download:
    repo_id: "${gemma_27b_q6.repo_id}"
    filename: "${gemma_27b_q6.filename}"
    model_dir: "${gemma.model_dir}"
    token: "${oc.env:HF_KEY}"

  infer:
    model_path: "${gemma.model_dir}/${gemma_27b_q6.filename}"
    <<: *std_infer
    <<: *gemma_infer

########################
# Mistral
########################

mistral:
  family: "mistral"
  data_dir: "${path.data_dir}/${mistral.family}"
  model_dir: "${path.model_dir}/${mistral.family}"

  infer: &mistral_infer
    class_name: "MistralLLM"
    chat_format: "mistral-instruct"
    stop:
      - "</s>"
      - "[INST]"
      - "[/INST]"
    temperature: 0.1

########################

mistral_31_24b_q6_k:
  repo_id: "unsloth/Mistral-Small-3.1-24B-Instruct-2503-GGUF"
  filename: "Mistral-Small-3.1-24B-Instruct-2503-Q6_K.gguf"
  ratings_path: "${mistral.data_dir}/ratings_mistral_31_24b_q6_k.csv"
  metrics_path: "${mistral.data_dir}/metrics_mistral_31_24b_q6_k.csv"

  download:
    repo_id: "${mistral_31_24b_q6_k.repo_id}"
    filename: "${mistral_31_24b_q6_k.filename}"
    model_dir: "${mistral.model_dir}"
    token: null

  infer:
    model_path: "${mistral.model_dir}/${mistral_31_24b_q6_k.filename}"
    <<: *std_infer
    <<: *mistral_infer

mistral_7b_q8:
  repo_id: "TheBloke/Mistral-7B-Instruct-v0.1-GGUF"
  filename: "mistral-7b-instruct-v0.1.Q8_0.gguf"
  ratings_path: "${mistral.data_dir}/ratings_mistral_7b_q8.csv"
  metrics_path: "${mistral.data_dir}/metrics_mistral_7b_q8.csv"

  download:
    repo_id: "${mistral_7b_q8.repo_id}"
    filename: "${mistral_7b_q8.filename}"
    model_dir: "${mistral.model_dir}"
    token: null

  infer:
    model_path: "${mistral.model_dir}/${mistral_7b_q8.filename}"
    <<: *std_infer
    <<: *mistral_infer

mistral_7b_q6:
  repo_id: "TheBloke/Mistral-7B-Instruct-v0.1-GGUF"
  filename: "mistral-7b-instruct-v0.1.Q6_K.gguf"
  ratings_path: "${mistral.data_dir}/ratings_mistral_7b_q6.csv"
  metrics_path: "${mistral.data_dir}/metrics_mistral_7b_q6.csv"

  download:
    repo_id: "${mistral_7b_q6.repo_id}"
    filename: "${mistral_7b_q6.filename}"
    model_dir: "${mistral.model_dir}"
    token: null

  infer:
    model_path: "${mistral.model_dir}/${mistral_7b_q6.filename}"
    <<: *std_infer
    <<: *mistral_infer

mistral_7b_q5_km:
  repo_id: "TheBloke/Mistral-7B-Instruct-v0.1-GGUF"
  filename: "mistral-7b-instruct-v0.1.Q5_K_M.gguf"
  ratings_path: "${mistral.data_dir}/ratings_mistral_7b_q5_km.csv"
  metrics_path: "${mistral.data_dir}/metrics_mistral_7b_q5_km.csv"

  download:
    repo_id: "${mistral_7b_q5_km.repo_id}"
    filename: "${mistral_7b_q5_km.filename}"
    model_dir: "${mistral.model_dir}"
    token: null

  infer:
    model_path: "${mistral.model_dir}/${mistral_7b_q5_km.filename}"
    <<: *std_infer
    <<: *mistral_infer

mistral_7b_q5_ks:
  repo_id: "TheBloke/Mistral-7B-Instruct-v0.1-GGUF"
  filename: "mistral-7b-instruct-v0.1.Q5_K_S.gguf"
  ratings_path: "${mistral.data_dir}/ratings_mistral_7b_q5_ks.csv"
  metrics_path: "${mistral.data_dir}/metrics_mistral_7b_q5_ks.csv"

  download:
    repo_id: "${mistral_7b_q5_ks.repo_id}"
    filename: "${mistral_7b_q5_ks.filename}"
    model_dir: "${mistral.model_dir}"
    token: null

  infer:
    model_path: "${mistral.model_dir}/${mistral_7b_q5_ks.filename}"
    <<: *std_infer
    <<: *mistral_infer

mistral_7b_q4_km:
  repo_id: "TheBloke/Mistral-7B-Instruct-v0.1-GGUF"
  filename: "mistral-7b-instruct-v0.1.Q4_K_M.gguf"
  ratings_path: "${mistral.data_dir}/ratings_mistral_7b_q4_km.csv"
  metrics_path: "${mistral.data_dir}/metrics_mistral_7b_q4_km.csv"

  download:
    repo_id: "${mistral_7b_q4_km.repo_id}"
    filename: "${mistral_7b_q4_km.filename}"
    model_dir: "${mistral.model_dir}"
    token: null

  infer:
    model_path: "${mistral.model_dir}/${mistral_7b_q4_km.filename}"
    <<: *std_infer
    <<: *mistral_infer


########################
# Llama 3
########################

llama:
  family: "llama"
  data_dir: "${path.data_dir}/${llama.family}"
  model_dir: "${path.model_dir}/${llama.family}"

  infer: &llama_infer
    class_name: "LlamaLLM"
    temperature: 0.2

########################

llama_31_8b:
  repo_id: "mradermacher/Llama-3.1-8B-SpecialTitanFusion-GGUF"
  filename: "Llama-3.1-8B-SpecialTitanFusion.Q8_0.gguf"
  ratings_path: "${llama.data_dir}/ratings_llama_31_8b.csv"
  metrics_path: "${llama.data_dir}/metrics_llama_31_8b.csv"

  download:
    repo_id: "${llama_31_8b.repo_id}"
    filename: "${llama_31_8b.filename}"
    model_dir: "${llama.model_dir}"
    token: null

  infer:
    model_path: "${llama.model_dir}/${llama_31_8b.filename}"
    <<: *std_infer
    <<: *llama_infer

llama_32_3b:
  repo_id: "bartowski/Llama-3.2-3B-Instruct-GGUF"
  filename: "Llama-3.2-3B-Instruct-Q8_0.gguf"
  ratings_path: "${llama.data_dir}/ratings_llama_32_3b.csv"
  metrics_path: "${llama.data_dir}/metrics_llama_32_3b.csv"

  download:
    repo_id: "${llama_32_3b.repo_id}"
    filename: "${llama_32_3b.filename}"
    model_dir: "${llama.model_dir}"
    token: null

  infer:
    model_path: "${llama.model_dir}/${llama_32_3b.filename}"
    <<: *std_infer
    <<: *llama_infer

llama_31_8b_q16:
  repo_id: "modularai/Llama-3.1-8B-Instruct-GGUF"
  filename: "llama-3.1-8b-instruct-bf16.gguf"
  ratings_path: "${llama.data_dir}/ratings_llama_31_8b_q16.csv"
  metrics_path: "${llama.data_dir}/metrics_llama_31_8b_q16.csv"

  download:
    repo_id: "${llama_31_8b_q16.repo_id}"
    filename: "${llama_31_8b_q16.filename}"
    model_dir: "${llama.model_dir}"
    token: null

  infer:
    model_path: "${llama.model_dir}/${llama_31_8b_q16.filename}"
    <<: *std_infer
    <<: *llama_infer

llama_32_1b_q16:
  repo_id: "unsloth/Llama-3.2-1B-Instruct-GGUF"
  filename: "Llama-3.2-1B-Instruct-BF16.gguf"
  ratings_path: "${llama.data_dir}/ratings_llama_32_1b_q16.csv"
  metrics_path: "${llama.data_dir}/metrics_llama_32_1b_q16.csv"

  download:
    repo_id: "${llama_32_1b_q16.repo_id}"
    filename: "${llama_32_1b_q16.filename}"
    model_dir: "${llama.model_dir}"
    token: null

  infer:
    model_path: "${llama.model_dir}/${llama_32_1b_q16.filename}"
    <<: *std_infer
    <<: *llama_infer

llama_32_3b_bf16:
  repo_id: "unsloth/Llama-3.2-3B-Instruct-GGUF"
  filename: "Llama-3.2-3B-Instruct-BF16.gguf"
  ratings_path: "${llama.data_dir}/ratings_llama_32_3b_bf16.csv"
  metrics_path: "${llama.data_dir}/metrics_llama_32_3b_bf16.csv"

  download:
    repo_id: "${llama_32_3b_bf16.repo_id}"
    filename: "${llama_32_3b_bf16.filename}"
    model_dir: "${llama.model_dir}"
    token: null

  infer:
    model_path: "${llama.model_dir}/${llama_32_3b_bf16.filename}"
    <<: *std_infer
    <<: *llama_infer

########################
# Qwen
########################

qwen:
  family: "qwen"
  data_dir: "${path.data_dir}/${qwen.family}"
  model_dir: "${path.model_dir}/${qwen.family}"

  infer: &qwen_infer
    class_name: "QwenLLM"
    chat_format: "chatml"
    rope_freq_base: 1000000.0
    temperature: 0.7
    top_p: 0.8
    top_k: 20
    min_p: 0
    max_tokens: 128
    repeat_penalty: 1.0

########################

qwen_3_8b_q4_km:
  repo_id: "mradermacher/Qwen3-8B-GGUF"
  filename: "Qwen3-8B.Q4_K_M.gguf"
  ratings_path: "${qwen.data_dir}/ratings_qwen_3_8b_q4_km.csv"
  metrics_path: "${qwen.data_dir}/metrics_qwen_3_8b_q4_km.csv"

  download:
    repo_id: "${qwen_3_8b_q4_km.repo_id}"
    filename: "${qwen_3_8b_q4_km.filename}"
    model_dir: "${qwen.model_dir}"
    token: null

  infer:
    model_path: "${qwen.model_dir}/${qwen_3_8b_q4_km.filename}"
    is_qwq: false
    <<: *std_infer
    <<: *qwen_infer

qwen_25_7b_q4_km:
  repo_id: "Qwen/Qwen2.5-Coder-7B-Instruct-GGUF"
  filename: "qwen2.5-coder-7b-instruct-q4_k_m.gguf"
  ratings_path: "${qwen.data_dir}/ratings_qwen_25_7b_q4_km.csv"
  metrics_path: "${qwen.data_dir}/metrics_qwen_25_7b_q4_km.csv"

  download:
    repo_id: "${qwen_25_7b_q4_km.repo_id}"
    filename: "${qwen_25_7b_q4_km.filename}"
    model_dir: "${qwen.model_dir}"
    token: null

  infer:
    model_path: "${qwen.model_dir}/${qwen_25_7b_q4_km.filename}"
    is_qwq: false
    <<: *std_infer
    <<: *qwen_infer

qwq_32b_q6_k:
  repo_id: "Qwen/QwQ-32B-GGUF"
  filename: "qwq-32b-q6_k.gguf"
  ratings_path: "${qwen.data_dir}/ratings_qwq_32b_q6_k.csv"
  metrics_path: "${qwen.data_dir}/metrics_qwq_32b_q6_k.csv"

  download:
    repo_id: "${qwq_32b_q6_k.repo_id}"
    filename: "${qwen_25_7qwq_32b_q6_kb_q4_km.filename}"
    model_dir: "${qwen.model_dir}"
    token: null

  infer:
    model_path: "${qwen.model_dir}/${qwq_32b_q6_k.filename}"
    is_qwq: true
    <<: *std_infer
    <<: *qwen_infer

qwen_3_8b_q4_km_unsloth:
  repo_id: "unsloth/Qwen3-8B-GGUF"
  filename: "Qwen3-8B-Q4_K_M.gguf"
  ratings_path: "${qwen.data_dir}/ratings_qwen_3_8b_q4_km_unsloth.csv"
  metrics_path: "${qwen.data_dir}/metrics_qwen_3_8b_q4_km_unsloth.csv"

  download:
    repo_id: "${qwen_3_8b_q4_km_unsloth.repo_id}"
    filename: "${qwen_3_8b_q4_km_unsloth.filename}"
    model_dir: "${qwen.model_dir}"
    token: null

  infer:
    model_path: "${qwen.model_dir}/${qwen_3_8b_q4_km_unsloth.filename}"
    <<: *std_infer
    <<: *qwen_infer

########################
# DeepSeek
########################

deepseek:
  family: "deepseek"
  
  data_dir: "${path.data_dir}/${deepseek.family}"
  model_dir: "${path.model_dir}/${deepseek.family}"

  infer: &deepseek_infer
    class_name: "DeepSeekLLM"
    stop:
      - "< | User | >"
    temperature: 0.6
    top_p: 0.95

########################

deepseek_r1_llama_8b:
  repo_id: "unsloth/DeepSeek-R1-Distill-Llama-8B-GGUF"
  filename: "DeepSeek-R1-Distill-Llama-8B-Q6_K.gguf"
  ratings_path: "${deepseek.data_dir}/ratings_deepseek_r1_llama_8b.csv"
  metrics_path: "${deepseek.data_dir}/metrics_deepseek_r1_llama_8b.csv"

  download:
    repo_id: "${deepseek_r1_llama_8b.repo_id}"
    filename: "${deepseek_r1_llama_8b.filename}"
    model_dir: "${deepseek.model_dir}"
    token: null

  infer:
    model_path: "${deepseek.model_dir}/${deepseek_r1_llama_8b.filename}"
    <<: *std_infer
    <<: *deepseek_infer

deepseek_r1_qwen_14b_q8:
  repo_id: "bartowski/DeepSeek-R1-Distill-Qwen-14B-GGUF"
  filename: "DeepSeek-R1-Distill-Qwen-14B-Q8_0.gguf"
  ratings_path: "${deepseek.data_dir}/ratings_deepseek_r1_qwen_14b_q8.csv"
  metrics_path: "${deepseek.data_dir}/metrics_deepseek_r1_qwen_14b_q8.csv"

  download:
    repo_id: "${deepseek_r1_qwen_14b_q8.repo_id}"
    filename: "${deepseek_r1_qwen_14b_q8.filename}"
    model_dir: "${deepseek.model_dir}"
    token: null

  infer:
    model_path: "${deepseek.model_dir}/${deepseek_r1_qwen_14b_q8.filename}"
    <<: *std_infer
    <<: *deepseek_infer

deepseek_r1_qwen_7b_q8:
  repo_id: "lmstudio-community/DeepSeek-R1-Distill-Qwen-7B-GGUF"
  filename: "DeepSeek-R1-Distill-Qwen-7B-Q8_0.gguf"
  ratings_path: "${deepseek.data_dir}/ratings_deepseek_r1_qwen_7b_q8.csv"
  metrics_path: "${deepseek.data_dir}/metrics_deepseek_r1_qwen_7b_q8.csv"

  download:
    repo_id: "${deepseek_r1_qwen_7b_q8.repo_id}"
    filename: "${deepseek_r1_qwen_7b_q8.filename}"
    model_dir: "${deepseek.model_dir}"
    token: null

  infer:
    model_path: "${deepseek.model_dir}/${deepseek_r1_qwen_7b_q8.filename}"
    <<: *std_infer
    <<: *deepseek_infer

deepseek_3_33b_q4_km:
  repo_id: "mradermacher/DeepSeek-33B-NL2SQL-GGUF"
  filename: "DeepSeek-33B-NL2SQL.Q4_K_M.gguf"
  ratings_path: "${deepseek.data_dir}/ratings_deepseek_3_33b_q4_km.csv"
  metrics_path: "${deepseek.data_dir}/metrics_deepseek_3_33b_q4_km.csv"

  download:
    repo_id: "${deepseek_3_33b_q4_km.repo_id}"
    filename: "${deepseek_3_33b_q4_km.filename}"
    model_dir: "${deepseek.model_dir}"
    token: null

  infer:
    model_path: "${deepseek.model_dir}/${deepseek_3_33b_q4_km.filename}"
    <<: *std_infer
    <<: *deepseek_infer

########################
# Phi
########################

phi:
  family: "phi"
  
  data_dir: "${path.data_dir}/${phi.family}"
  model_dir: "${path.model_dir}/${phi.family}"

  infer: &phi_infer
    class_name: "LlamaLLM"
    temperature: 0.1

########################

phi_4_14b_q8_unsloth:
  repo_id: "unsloth/Phi-4-reasoning-plus-GGUF"
  filename: "unsloth/Phi-4-reasoning-plus-GGUF" # 15.6GB
  ratings_path: "${deepseek.data_dir}/ratings_phi_4_14b_q8_unsloth.csv"
  metrics_path: "${deepseek.data_dir}/metrics_phi_4_14b_q8_unsloth.csv"

  download:
    repo_id: "${phi_4_14b_q8_unsloth.repo_id}"
    filename: "${phi_4_14b_q8_unsloth.filename}"
    model_dir: "${deepseek.model_dir}"
    token: null

  infer:
    model_path: "${deepseek.model_dir}/${phi_4_14b_q8_unsloth.filename}"
    <<: *std_infer
    <<: *phi_infer

